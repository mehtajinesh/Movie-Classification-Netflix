{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. General dataset exploration\n",
    "2. Basic data visualisation\n",
    "3. Tokenize text data\n",
    "4. Build autencoder and clustering layer\n",
    "5. Visualize the cluster with Seaborn\n",
    "6. Interactive scatterplot with Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset and Validate 5 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>s814</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>The Adventures of Sonic the Hedgehog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jaleel White, Long John Baldry, Garry Chalk</td>\n",
       "      <td>United States, Canada</td>\n",
       "      <td>June 2, 2021</td>\n",
       "      <td>1993</td>\n",
       "      <td>TV-Y7</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Kids' TV</td>\n",
       "      <td>Hyper hedgehog Sonic and his cohort Miles \"Tai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>s5583</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Amy Schumer: The Leather Special</td>\n",
       "      <td>Amy Schumer</td>\n",
       "      <td>Amy Schumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>March 7, 2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>57 min</td>\n",
       "      <td>Stand-Up Comedy</td>\n",
       "      <td>Comic sensation Amy Schumer riffs on sex, dati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>s290</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>The Crowned Clown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeo Jin-goo, Lee Se-young, Kim Sang-kyung, Jun...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>August 10, 2021</td>\n",
       "      <td>2019</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>International TV Shows, Romantic TV Shows, TV ...</td>\n",
       "      <td>Standing in for an unhinged Joseon king, a loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>s5273</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>VeggieTales in the City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phil Vischer, Mike Nawrocki, Rob Paulsen, Tres...</td>\n",
       "      <td>United States</td>\n",
       "      <td>September 15, 2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>TV-Y</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>Kids' TV</td>\n",
       "      <td>With exciting trips to the big city, the ski s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>s1174</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Men on a Mission</td>\n",
       "      <td>Jung-ah Im</td>\n",
       "      <td>Ho-dong Kang, Soo-geun Lee, Sang-min Lee, Youn...</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>March 23, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>6 Seasons</td>\n",
       "      <td>International TV Shows, Korean TV Shows, Stand...</td>\n",
       "      <td>Male celebs play make-believe as high schooler...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     show_id     type                                 title     director  \\\n",
       "813     s814  TV Show  The Adventures of Sonic the Hedgehog          NaN   \n",
       "5582   s5583    Movie      Amy Schumer: The Leather Special  Amy Schumer   \n",
       "289     s290  TV Show                     The Crowned Clown          NaN   \n",
       "5272   s5273  TV Show               VeggieTales in the City          NaN   \n",
       "1173   s1174  TV Show                      Men on a Mission   Jung-ah Im   \n",
       "\n",
       "                                                   cast  \\\n",
       "813         Jaleel White, Long John Baldry, Garry Chalk   \n",
       "5582                                        Amy Schumer   \n",
       "289   Yeo Jin-goo, Lee Se-young, Kim Sang-kyung, Jun...   \n",
       "5272  Phil Vischer, Mike Nawrocki, Rob Paulsen, Tres...   \n",
       "1173  Ho-dong Kang, Soo-geun Lee, Sang-min Lee, Youn...   \n",
       "\n",
       "                    country          date_added  release_year rating  \\\n",
       "813   United States, Canada        June 2, 2021          1993  TV-Y7   \n",
       "5582          United States       March 7, 2017          2017  TV-MA   \n",
       "289             South Korea     August 10, 2021          2019  TV-14   \n",
       "5272          United States  September 15, 2017          2017   TV-Y   \n",
       "1173            South Korea      March 23, 2021          2021  TV-14   \n",
       "\n",
       "       duration                                          listed_in  \\\n",
       "813    1 Season                                           Kids' TV   \n",
       "5582     57 min                                    Stand-Up Comedy   \n",
       "289    1 Season  International TV Shows, Romantic TV Shows, TV ...   \n",
       "5272  2 Seasons                                           Kids' TV   \n",
       "1173  6 Seasons  International TV Shows, Korean TV Shows, Stand...   \n",
       "\n",
       "                                            description  \n",
       "813   Hyper hedgehog Sonic and his cohort Miles \"Tai...  \n",
       "5582  Comic sensation Amy Schumer riffs on sex, dati...  \n",
       "289   Standing in for an unhinged Joseon king, a loo...  \n",
       "5272  With exciting trips to the big city, the ski s...  \n",
       "1173  Male celebs play make-believe as high schooler...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIR_NETFLIX = \"netflix_titles.csv\"\n",
    "\n",
    "df_netflix = pd.read_csv(INPUT_DIR_NETFLIX)\n",
    "df_netflix.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing and Fetching Movie Data\n",
    "\n",
    "1. Remove duplications\n",
    "2. Replace missing director data with 'no data'\n",
    "3. Drop NA records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id         0\n",
      "type            0\n",
      "title           0\n",
      "director        0\n",
      "cast            0\n",
      "country         0\n",
      "date_added      0\n",
      "release_year    0\n",
      "rating          0\n",
      "duration        0\n",
      "listed_in       0\n",
      "description     0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5277 entries, 7 to 8806\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   show_id       5277 non-null   object\n",
      " 1   type          5277 non-null   object\n",
      " 2   title         5277 non-null   object\n",
      " 3   director      5277 non-null   object\n",
      " 4   cast          5277 non-null   object\n",
      " 5   country       5277 non-null   object\n",
      " 6   date_added    5277 non-null   object\n",
      " 7   release_year  5277 non-null   int64 \n",
      " 8   rating        5277 non-null   object\n",
      " 9   duration      5277 non-null   object\n",
      " 10  listed_in     5277 non-null   object\n",
      " 11  description   5277 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 535.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_netflix['director'].replace(np.nan, 'No Data',inplace  = True)\n",
    "\n",
    "# Drops\n",
    "\n",
    "df_netflix.dropna(inplace=True)\n",
    "\n",
    "# Drop Duplicates\n",
    "\n",
    "df_netflix.drop_duplicates(inplace= True)\n",
    "\n",
    "df_movies_netflix = df_netflix.loc[df_netflix['type'] == 'Movie']\n",
    "\n",
    "# verify dataframe\n",
    "print(df_movies_netflix.isnull().sum())\n",
    "print(df_movies_netflix.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Clustering\n",
    "\n",
    "- Lets start with just the description.\n",
    "\n",
    "- Preprocess and tokenize the description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = df_movies_netflix['description']\n",
    "vector = TfidfVectorizer(max_df = 0.4,           # drop words that occur more than max_df %\n",
    "                         stop_words = 'english', # remove stop words\n",
    "                         lowercase = True,       # everything to lowercase\n",
    "                         use_idf = True,\n",
    "                         norm = u'l2',\n",
    "                         smooth_idf = True       # prevent divide by zero errors\n",
    "                         )\n",
    "\n",
    "tfidf = vector.fit_transform(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-0b88812b299f>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_movies_netflix['cluster'] = kmeans.predict(request_transform)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    4546\n",
       "4     481\n",
       "2     224\n",
       "1      25\n",
       "0       1\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "kmeans = MiniBatchKMeans(n_clusters = k)\n",
    "kmeans.fit(tfidf)\n",
    "centres = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vector.get_feature_names()\n",
    "\n",
    "request_transform = vector.transform(df_movies_netflix['description'])\n",
    "df_movies_netflix['cluster'] = kmeans.predict(request_transform)\n",
    "df_movies_netflix['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_clustered_labels = to_categorical(df_movies_netflix['cluster'], num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_movies_netflix['description'],\n",
    "                                                    categorized_clustered_labels, \n",
    "                                                    test_size = 0.10, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "shape of X_train_ids: (4749, 512)\n",
      "--------------------------------------------------\n",
      "shape of X_train_mask: (4749, 512)\n",
      "==================================================\n",
      "==================================================\n",
      "shape of X_test_ids: (528, 512)\n",
      "--------------------------------------------------\n",
      "shape of X_test_mask: (528, 512)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, \n",
    "                                 max_length=512,\n",
    "                                 truncation=True, \n",
    "                                 padding='max_length',\n",
    "                                 add_special_tokens=True, \n",
    "                                 return_token_type_ids=False,\n",
    "                                 return_tensors='tf')  \n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "\n",
    "X_train_ids = np.zeros((len(X_train), 512))\n",
    "X_train_mask = np.zeros((len(X_train), 512))\n",
    "X_test_ids = np.zeros((len(X_test), 512))\n",
    "X_test_mask = np.zeros((len(X_test), 512))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"shape of X_train_ids:\",X_train_ids.shape)\n",
    "print(\"-\"*50)\n",
    "print(\"shape of X_train_mask:\",X_train_mask.shape)\n",
    "print(\"=\"*50)\n",
    "print(\"=\"*50)\n",
    "print(\"shape of X_test_ids:\",X_test_ids.shape)\n",
    "print(\"-\"*50)\n",
    "print(\"shape of X_test_mask:\",X_test_mask.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, sequence in enumerate(X_train):\n",
    "    tokens = tokenize(sequence)\n",
    "    X_train_ids[i, :], X_train_mask[i, :] = tokens[0], tokens[1]\n",
    "    \n",
    "for i, sequence in enumerate(X_test):\n",
    "    tokens = tokenize(sequence)\n",
    "    X_test_ids[i, :], X_test_mask[i, :] = tokens[0], tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the tokenized ids and Mask into tensorflow Tensors\n",
    "X_train_ids = tf.convert_to_tensor(X_train_ids)\n",
    "X_train_mask = tf.convert_to_tensor(X_train_mask)\n",
    "\n",
    "X_test_ids = tf.convert_to_tensor(X_test_ids)\n",
    "X_test_mask = tf.convert_to_tensor(X_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = tf.data.Dataset.from_tensor_slices((X_train_ids, \n",
    "                                                 X_train_mask, \n",
    "                                                 y_train))\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices((X_test_ids, \n",
    "                                                X_test_mask, \n",
    "                                                y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE = 100000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "    \n",
    "data_train = data_train.map(map_func)\n",
    "data_test = data_test.map(map_func)\n",
    "\n",
    "train = data_train.shuffle(SHUFFLE).batch(BATCH_SIZE) #, drop_remainder=True)\n",
    "val = data_test.shuffle(SHUFFLE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "bert = TFAutoModel.from_pretrained('bert-base-uncased')\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[0]  # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.4)(embeddings)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768))(x)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.8585 - f1_m: 0.8572 - precision_m: 0.8590 - recall_m: 0.8554\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83712, saving model to best_model.hdf5\n",
      "149/149 [==============================] - 115s 685ms/step - loss: 0.5013 - accuracy: 0.8585 - f1_m: 0.8572 - precision_m: 0.8590 - recall_m: 0.8554 - val_loss: 0.4799 - val_accuracy: 0.8371 - val_f1_m: 0.8442 - val_precision_m: 0.8465 - val_recall_m: 0.8419\n",
      "Epoch 2/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.3699 - accuracy: 0.8825 - f1_m: 0.8823 - precision_m: 0.8882 - recall_m: 0.8766\n",
      "Epoch 2: val_accuracy improved from 0.83712 to 0.91856, saving model to best_model.hdf5\n",
      "149/149 [==============================] - 99s 664ms/step - loss: 0.3699 - accuracy: 0.8825 - f1_m: 0.8823 - precision_m: 0.8882 - recall_m: 0.8766 - val_loss: 0.2705 - val_accuracy: 0.9186 - val_f1_m: 0.9122 - val_precision_m: 0.9263 - val_recall_m: 0.8989\n",
      "Epoch 3/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9398 - f1_m: 0.9400 - precision_m: 0.9423 - recall_m: 0.9377\n",
      "Epoch 3: val_accuracy improved from 0.91856 to 0.97538, saving model to best_model.hdf5\n",
      "149/149 [==============================] - 99s 664ms/step - loss: 0.1931 - accuracy: 0.9398 - f1_m: 0.9400 - precision_m: 0.9423 - recall_m: 0.9377 - val_loss: 0.1347 - val_accuracy: 0.9754 - val_f1_m: 0.9741 - val_precision_m: 0.9759 - val_recall_m: 0.9724\n",
      "Epoch 4/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9697 - f1_m: 0.9701 - precision_m: 0.9708 - recall_m: 0.9694\n",
      "Epoch 4: val_accuracy improved from 0.97538 to 0.98295, saving model to best_model.hdf5\n",
      "149/149 [==============================] - 99s 665ms/step - loss: 0.1059 - accuracy: 0.9697 - f1_m: 0.9701 - precision_m: 0.9708 - recall_m: 0.9694 - val_loss: 0.0649 - val_accuracy: 0.9830 - val_f1_m: 0.9835 - val_precision_m: 0.9835 - val_recall_m: 0.9835\n",
      "Epoch 5/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9844 - f1_m: 0.9846 - precision_m: 0.9849 - recall_m: 0.9843\n",
      "Epoch 5: val_accuracy did not improve from 0.98295\n",
      "149/149 [==============================] - 97s 653ms/step - loss: 0.0617 - accuracy: 0.9844 - f1_m: 0.9846 - precision_m: 0.9849 - recall_m: 0.9843 - val_loss: 0.0528 - val_accuracy: 0.9830 - val_f1_m: 0.9835 - val_precision_m: 0.9835 - val_recall_m: 0.9835\n",
      "Epoch 6/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9859 - f1_m: 0.9857 - precision_m: 0.9863 - recall_m: 0.9851\n",
      "Epoch 6: val_accuracy improved from 0.98295 to 0.99242, saving model to best_model.hdf5\n",
      "149/149 [==============================] - 99s 665ms/step - loss: 0.0514 - accuracy: 0.9859 - f1_m: 0.9857 - precision_m: 0.9863 - recall_m: 0.9851 - val_loss: 0.0306 - val_accuracy: 0.9924 - val_f1_m: 0.9908 - val_precision_m: 0.9908 - val_recall_m: 0.9908\n",
      "Epoch 7/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9869 - f1_m: 0.9870 - precision_m: 0.9872 - recall_m: 0.9868\n",
      "Epoch 7: val_accuracy did not improve from 0.99242\n",
      "149/149 [==============================] - 97s 653ms/step - loss: 0.0425 - accuracy: 0.9869 - f1_m: 0.9870 - precision_m: 0.9872 - recall_m: 0.9868 - val_loss: 0.0801 - val_accuracy: 0.9867 - val_f1_m: 0.9871 - val_precision_m: 0.9871 - val_recall_m: 0.9871\n",
      "Epoch 8/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9859 - f1_m: 0.9860 - precision_m: 0.9864 - recall_m: 0.9857\n",
      "Epoch 8: val_accuracy did not improve from 0.99242\n",
      "149/149 [==============================] - 97s 654ms/step - loss: 0.0453 - accuracy: 0.9859 - f1_m: 0.9860 - precision_m: 0.9864 - recall_m: 0.9857 - val_loss: 0.0446 - val_accuracy: 0.9886 - val_f1_m: 0.9890 - val_precision_m: 0.9890 - val_recall_m: 0.9890\n",
      "Epoch 9/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9899 - f1_m: 0.9890 - precision_m: 0.9893 - recall_m: 0.9887\n",
      "Epoch 9: val_accuracy did not improve from 0.99242\n",
      "149/149 [==============================] - 97s 654ms/step - loss: 0.0343 - accuracy: 0.9899 - f1_m: 0.9890 - precision_m: 0.9893 - recall_m: 0.9887 - val_loss: 0.0654 - val_accuracy: 0.9792 - val_f1_m: 0.9798 - val_precision_m: 0.9798 - val_recall_m: 0.9798\n",
      "Epoch 9: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc, f1_m,precision_m, recall_m])\n",
    "\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                               min_delta=0,\n",
    "                               patience=3,\n",
    "                               verbose=2, \n",
    "                               mode='max')\n",
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\",\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True, \n",
    "                             mode='max', \n",
    "                             period=1)\n",
    "callbacks_1 = [early_stopping,checkpoint]\n",
    "\n",
    "history = model.fit(train,\n",
    "                    validation_data=val,\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
