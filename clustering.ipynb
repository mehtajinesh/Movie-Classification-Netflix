{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. General dataset exploration\n",
    "2. Basic data visualisation\n",
    "3. Tokenize text data\n",
    "4. Build autencoder and clustering layer\n",
    "5. Visualize the cluster with Seaborn\n",
    "6. Interactive scatterplot with Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset and Validate 5 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>s168</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Open Season 2</td>\n",
       "      <td>Matthew O'Callaghan, Todd Wilderman</td>\n",
       "      <td>Joel McHale, Mike Epps, Jane Krakowski, Billy ...</td>\n",
       "      <td>United States, Canada</td>\n",
       "      <td>September 1, 2021</td>\n",
       "      <td>2008</td>\n",
       "      <td>PG</td>\n",
       "      <td>76 min</td>\n",
       "      <td>Children &amp; Family Movies, Comedies</td>\n",
       "      <td>Elliot the buck and his forest-dwelling cohort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>s5323</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Undefeated</td>\n",
       "      <td>Daniel Lindsay, T.J. Martin</td>\n",
       "      <td>Montrail 'Money' Brown, O.C. Brown, Bill Court...</td>\n",
       "      <td>United States</td>\n",
       "      <td>August 18, 2017</td>\n",
       "      <td>2011</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>113 min</td>\n",
       "      <td>Documentaries, Sports Movies</td>\n",
       "      <td>An inspirational profile of an inner-city high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>s2801</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Masameer - The Movie</td>\n",
       "      <td>Malik Nejer</td>\n",
       "      <td>Malik Nejer, Shahad Alahmari, Abdulaziz Almuza...</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>March 19, 2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>100 min</td>\n",
       "      <td>Comedies, International Movies</td>\n",
       "      <td>A young girl passionate about AI sets out to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7947</th>\n",
       "      <td>s7948</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Savage Raghda</td>\n",
       "      <td>Mahmoud Karim</td>\n",
       "      <td>Ramez Galal, Riham Hagag, Bayyumi Fuad, Entess...</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>June 13, 2019</td>\n",
       "      <td>2018</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>93 min</td>\n",
       "      <td>Comedies, International Movies, Romantic Movies</td>\n",
       "      <td>Desperate to support his son, a single father ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>s655</td>\n",
       "      <td>Movie</td>\n",
       "      <td>#Selfie</td>\n",
       "      <td>Cristina Jacob</td>\n",
       "      <td>Flavia Hojda, Crina Semciuc, Olimpia Melinte, ...</td>\n",
       "      <td>Romania</td>\n",
       "      <td>June 21, 2021</td>\n",
       "      <td>2014</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>125 min</td>\n",
       "      <td>Comedies, Dramas, International Movies</td>\n",
       "      <td>Two days before their final exams, three teen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     show_id   type                 title  \\\n",
       "167     s168  Movie         Open Season 2   \n",
       "5322   s5323  Movie            Undefeated   \n",
       "2800   s2801  Movie  Masameer - The Movie   \n",
       "7947   s7948  Movie         Savage Raghda   \n",
       "654     s655  Movie               #Selfie   \n",
       "\n",
       "                                 director  \\\n",
       "167   Matthew O'Callaghan, Todd Wilderman   \n",
       "5322          Daniel Lindsay, T.J. Martin   \n",
       "2800                          Malik Nejer   \n",
       "7947                        Mahmoud Karim   \n",
       "654                        Cristina Jacob   \n",
       "\n",
       "                                                   cast  \\\n",
       "167   Joel McHale, Mike Epps, Jane Krakowski, Billy ...   \n",
       "5322  Montrail 'Money' Brown, O.C. Brown, Bill Court...   \n",
       "2800  Malik Nejer, Shahad Alahmari, Abdulaziz Almuza...   \n",
       "7947  Ramez Galal, Riham Hagag, Bayyumi Fuad, Entess...   \n",
       "654   Flavia Hojda, Crina Semciuc, Olimpia Melinte, ...   \n",
       "\n",
       "                    country         date_added  release_year rating duration  \\\n",
       "167   United States, Canada  September 1, 2021          2008     PG   76 min   \n",
       "5322          United States    August 18, 2017          2011  PG-13  113 min   \n",
       "2800           Saudi Arabia     March 19, 2020          2020  TV-14  100 min   \n",
       "7947                  Egypt      June 13, 2019          2018  TV-MA   93 min   \n",
       "654                 Romania      June 21, 2021          2014  TV-MA  125 min   \n",
       "\n",
       "                                            listed_in  \\\n",
       "167                Children & Family Movies, Comedies   \n",
       "5322                     Documentaries, Sports Movies   \n",
       "2800                   Comedies, International Movies   \n",
       "7947  Comedies, International Movies, Romantic Movies   \n",
       "654            Comedies, Dramas, International Movies   \n",
       "\n",
       "                                            description  \n",
       "167   Elliot the buck and his forest-dwelling cohort...  \n",
       "5322  An inspirational profile of an inner-city high...  \n",
       "2800  A young girl passionate about AI sets out to m...  \n",
       "7947  Desperate to support his son, a single father ...  \n",
       "654   Two days before their final exams, three teen ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIR_NETFLIX = \"data/netflix_titles.csv\"\n",
    "\n",
    "df_netflix = pd.read_csv(INPUT_DIR_NETFLIX)\n",
    "df_netflix.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing and Fetching Movie Data\n",
    "\n",
    "1. Remove duplications\n",
    "2. Replace missing director data with 'no data'\n",
    "3. Drop NA records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id         0\n",
      "type            0\n",
      "title           0\n",
      "director        0\n",
      "cast            0\n",
      "country         0\n",
      "date_added      0\n",
      "release_year    0\n",
      "rating          0\n",
      "duration        0\n",
      "listed_in       0\n",
      "description     0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5277 entries, 7 to 8806\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   show_id       5277 non-null   object\n",
      " 1   type          5277 non-null   object\n",
      " 2   title         5277 non-null   object\n",
      " 3   director      5277 non-null   object\n",
      " 4   cast          5277 non-null   object\n",
      " 5   country       5277 non-null   object\n",
      " 6   date_added    5277 non-null   object\n",
      " 7   release_year  5277 non-null   int64 \n",
      " 8   rating        5277 non-null   object\n",
      " 9   duration      5277 non-null   object\n",
      " 10  listed_in     5277 non-null   object\n",
      " 11  description   5277 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 535.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_netflix['director'].replace(np.nan, 'No Data',inplace  = True)\n",
    "\n",
    "# Drops\n",
    "\n",
    "df_netflix.dropna(inplace=True)\n",
    "\n",
    "# Drop Duplicates\n",
    "\n",
    "df_netflix.drop_duplicates(inplace= True)\n",
    "\n",
    "df_movies_netflix = df_netflix.loc[df_netflix['type'] == 'Movie']\n",
    "\n",
    "# verify dataframe\n",
    "print(df_movies_netflix.isnull().sum())\n",
    "print(df_movies_netflix.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Clustering\n",
    "\n",
    "- Lets start with just the description.\n",
    "\n",
    "- Preprocess and tokenize the description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15154 unique tokens found - vocabulary size\n",
      "[[   0    0    0 ...    8 1900   94]\n",
      " [   0    0    0 ...    1  111 1902]\n",
      " [   0    0    0 ...   25  593   52]\n",
      " ...\n",
      " [   0    0    0 ...    5 6768  509]\n",
      " [   0    0    0 ...    1 3125 1588]\n",
      " [   0    0    0 ...   51    7   94]]\n"
     ]
    }
   ],
   "source": [
    "df_token = df_movies_netflix[ \"description\"]\n",
    "\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_token)\n",
    "# transforms strings in list of intergers\n",
    "sequences = tokenizer.texts_to_sequences(df_token)\n",
    "# calculated word index / vocabulary size\n",
    "word_index = tokenizer.word_index \n",
    "print(f\"{len(word_index)} unique tokens found - vocabulary size\")\n",
    "data = pad_sequences(sequences) #transforms integer lists into 2D tensor\n",
    "print(data)\n",
    "# display(HTML(df_movies_netflix.head().to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling\n",
    "\n",
    "- Rescale the data to 0-1 range so that convergance happens faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "# the values of all features are rescaled into the range of [0, 1]\n",
    "x = scaler.fit_transform(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autoencoder for encoding word vectors + Clustering layer for generating labels [Also known as DEC - Deep Embedded Clustering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected symmetric auto-encoder model.\n",
    "  \n",
    "    dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n",
    "          dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n",
    "\n",
    "    act: activation function\n",
    "    \n",
    "    return:\n",
    "        (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    \n",
    "    input_data = Input(shape=(dims[0],), name='input')\n",
    "    x = input_data\n",
    "    \n",
    "    # internal layers of encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # latent hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers of decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # decoder output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    \n",
    "    decoded = x\n",
    "    \n",
    "    autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n",
    "    encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n",
    "    \n",
    "    return autoencoder_model, encoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max numbers of clusters\n",
    "n_clusters = 20\n",
    "# epchos for autencoder training \n",
    "n_epochs   = 10 \n",
    "# batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "\n",
    "dims = [x.shape[-1], 500, 500, 1000, 10] \n",
    "# this helps with better initialization of the weights\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                           distribution='uniform')\n",
    "# [rmsprop] Standard values for lr and momentum - SGD(lr=1, momentum=0.9)\n",
    "pretrain_optimizer = \"rmsprop\" \n",
    "pretrain_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define clustering layer which can be placed inside the pipeline to generate labels parallelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow.keras.backend as K_end\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    '''\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K_end.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n",
    "        \n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        ''' \n",
    "        student t-distribution, as used in t-SNE algorithm.\n",
    "        It measures the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "       \n",
    "        inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        \n",
    "        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        '''\n",
    "        q = 1.0 / (1.0 + (K_end.sum(K_end.square(K_end.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K_end.transpose(K_end.transpose(q) / K_end.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n",
    "        \n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and generate autoencoder and encoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5277\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x294d339d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x294d339d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.2508\n",
      "Epoch 2/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1995\n",
      "Epoch 3/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1938\n",
      "Epoch 4/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1929\n",
      "Epoch 5/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1910\n",
      "Epoch 6/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1884\n",
      "Epoch 7/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1938\n",
      "Epoch 8/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1936\n",
      "Epoch 9/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1906\n",
      "Epoch 10/10\n",
      "165/165 [==============================] - 1s 5ms/step - loss: 0.1934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x294dd9bb0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss='mse'\n",
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='binary_crossentropy')\n",
    "print(len(x))\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld') #(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2ab2670d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2ab2670d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 1000 # 8000\n",
    "update_interval = 100 # 140\n",
    "index_array = np.arange(x.shape[0])\n",
    "tol = 0.001 # tolerance threshold to stop training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2ab011e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x2ab011e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab40ed30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2ab40ed30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval.\n",
    "q = model.predict(x, verbose=0)\n",
    "p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = df_netflix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "x_embedded = TSNE(n_components=2).fit_transform(x)\n",
    "\n",
    "x_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y_pred)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title(\"Netflix Movies and Tv Shows, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
